ext4: support block size != page size

From: Michael Halcrow <mhalcrow@google.com>

After enough time hemming and hawing over taking additional
dependencies on buffer head, I decided to go ahead and just do it to
see if it looks like a viable approach.

This patch built a kernel with block size == 1024. It's not
particularly efficient, in that it allocates a struct bio for every
buffer head. With some additional complexity, I probably could
coalesce several buffer heads for one page into a single bio
completion.

Ted, if you think this is a reasonable thing to do, please let me
know. You can also let me know if you think it's an unreasonable thing
to do.

Signed-off-by: Michael Halcrow <mhalcrow@google.com>
Signed-off-by: Theodore Ts'o <tytso@mit.edu>
---
 fs/buffer.c           |  23 +++++---
 fs/ext4/crypto.c      | 112 +++++++++++++++++++------------------
 fs/ext4/ext4.h        |  10 +++-
 fs/ext4/ext4_crypto.h |  15 +++--
 fs/ext4/inode.c       | 150 ++++++++++++++++++++++++++++++++++----------------
 fs/ext4/page-io.c     | 141 ++++++++++++++++++++++++++++++-----------------
 6 files changed, 286 insertions(+), 165 deletions(-)

diff --git a/fs/buffer.c b/fs/buffer.c
index f2dcb6b..2506de5 100644
--- a/fs/buffer.c
+++ b/fs/buffer.c
@@ -289,6 +289,22 @@ void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 	page = bh->b_page;
 	if (uptodate) {
 		set_buffer_uptodate(bh);
+		if (bh->b_private) {
+			/*
+			 * Encryption involves a different buffer head tracking
+			 * mechanism. There's an atomic counter with a shared
+			 * crypto context for each page. When that hits 0, then
+			 * the callback sets the page uptodate and unlocks the
+			 * page.
+			 */
+			struct bio *bio = (struct bio *)bh->b_private;
+
+			BUG_ON(!bio->bi_cb);
+			bio->bi_cb(bio, 0);
+			clear_buffer_async_read(bh);
+			unlock_buffer(bh);
+			goto out;
+		}
 	} else {
 		clear_buffer_uptodate(bh);
 		buffer_io_error(bh, ", async page read");
@@ -318,13 +334,6 @@ void end_buffer_async_read(struct buffer_head *bh, int uptodate)
 	bit_spin_unlock(BH_Uptodate_Lock, &first->b_state);
 	local_irq_restore(flags);
 
-	if (bh->b_private) {
-		struct bio *bio = (struct bio *)bh->b_private;
-		BUG_ON(!bio->bi_cb);
-		if (!bio->bi_cb(bio, !(page_uptodate && !PageError(page))))
-			goto out;
-	}
-
 	/*
 	 * If none of the buffers had errors and they are all
 	 * uptodate then we can set the page uptodate.
diff --git a/fs/ext4/crypto.c b/fs/ext4/crypto.c
index d655a64..6150574 100644
--- a/fs/ext4/crypto.c
+++ b/fs/ext4/crypto.c
@@ -14,6 +14,7 @@
  * derivation code implements an HKDF (see RFC 5869).
  */
 
+#include <crypto/aes.h>
 #include <crypto/hash.h>
 #include <crypto/sha.h>
 #include <keys/user-type.h>
@@ -167,6 +168,8 @@ struct ext4_crypto_ctx *ext4_get_crypto_ctx(
 	}
 	BUG_ON(key->size != ext4_encryption_key_size(key->mode));
 
+	mutex_init(&ctx->tfm_mutex);
+
 	/* There shouldn't be a bounce page attached to the crypto
 	 * context at this point. */
 	BUG_ON(ctx->bounce_page);
@@ -283,22 +286,25 @@ fail:
 }
 
 /**
- * ext4_xts_tweak_for_page() - Generates an XTS tweak for a page
+ * ext4_xts_tweak() - Generates an XTS tweak
  * @xts_tweak: Buffer into which this writes the XTS tweak.
  * @page:      The page for which this generates a tweak.
+ * @offset:    The offset within the page.
  *
- * Generates an XTS tweak value for the given page.
+ * Generates an XTS tweak value for the given page and offset within the page.
  */
-static void ext4_xts_tweak_for_page(u8 xts_tweak[EXT4_XTS_TWEAK_SIZE],
-				    const struct page *page)
+static void ext4_xts_tweak(u8 xts_tweak[EXT4_XTS_TWEAK_SIZE],
+			   const struct page *page, size_t offset)
 {
+	size_t logical_offset = (page->index << PAGE_CACHE_SHIFT) + offset;
+
 	/* Only do this for XTS tweak values. For other modes (CBC,
 	 * GCM, etc.), you most like will need to do something
 	 * different. */
-	BUILD_BUG_ON(EXT4_XTS_TWEAK_SIZE < sizeof(page->index));
-	memcpy(xts_tweak, &page->index, sizeof(page->index));
-	memset(&xts_tweak[sizeof(page->index)], 0,
-	       EXT4_XTS_TWEAK_SIZE - sizeof(page->index));
+	BUILD_BUG_ON(EXT4_XTS_TWEAK_SIZE < sizeof(offset));
+	memcpy(xts_tweak, &logical_offset, sizeof(logical_offset));
+	memset(&xts_tweak[sizeof(logical_offset)], 0,
+	       EXT4_XTS_TWEAK_SIZE - sizeof(logical_offset));
 }
 
 /**
@@ -345,9 +351,9 @@ static void ext4_crypt_complete(struct crypto_async_request *req, int res)
  * @plaintext_page:  Plaintext page that acts as a control page.
  * @ctx:             Encryption context for the pages.
  */
-static void ext4_prep_pages_for_write(struct page *ciphertext_page,
-				      struct page *plaintext_page,
-				      struct ext4_crypto_ctx *ctx)
+void ext4_prep_pages_for_write(struct page *ciphertext_page,
+			       struct page *plaintext_page,
+			       struct ext4_crypto_ctx *ctx)
 {
 	SetPageDirty(ciphertext_page);
 	SetPagePrivate(ciphertext_page);
@@ -360,17 +366,18 @@ static void ext4_prep_pages_for_write(struct page *ciphertext_page,
  * ext4_xts_encrypt() - Encrypts a page using AES-256-XTS
  * @ctx:            The encryption context.
  * @plaintext_page: The page to encrypt. Must be locked.
+ * @offset:         The offset within the page to encrypt.
+ * @size:           The size of the region to encrypt.
  *
- * Allocates a ciphertext page and encrypts plaintext_page into it using the ctx
- * encryption context. Uses AES-256-XTS.
+ * Encrypts plaintext_page into the ctx bounce page using the ctx encryption
+ * context. Uses AES-256-XTS.
  *
  * Called on the page write path.
  *
- * Return: An allocated page with the encrypted content on success. Else, an
- * error value or NULL.
+ * Return: Zero on success; non-zero otherwise.
  */
-static struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
-				     struct page *plaintext_page)
+int ext4_xts_encrypt(struct ext4_crypto_ctx *ctx, struct page *plaintext_page,
+		     size_t offset, size_t size)
 {
 	struct page *ciphertext_page = ctx->bounce_page;
 	u8 xts_tweak[EXT4_XTS_TWEAK_SIZE];
@@ -384,6 +391,10 @@ static struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
 	BUG_ON(!ciphertext_page);
 	BUG_ON(!ctx->tfm);
 	BUG_ON(ei->i_encryption_key.mode != EXT4_ENCRYPTION_MODE_AES_256_XTS);
+	BUG_ON(offset % AES_BLOCK_SIZE != 0);
+	BUG_ON(size % AES_BLOCK_SIZE != 0);
+	BUG_ON(offset + size > PAGE_CACHE_SIZE);
+
 	crypto_ablkcipher_clear_flags(atfm, ~0);
 	crypto_tfm_set_flags(ctx->tfm, CRYPTO_TFM_REQ_WEAK_KEY);
 
@@ -392,24 +403,22 @@ static struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
 	 * single key can encrypt, we directly use the inode master key */
 	res = crypto_ablkcipher_setkey(atfm, ei->i_encryption_key.raw,
 				       ei->i_encryption_key.size);
+	if (IS_ERR_VALUE(res))
+		goto out;
 	req = ablkcipher_request_alloc(atfm, GFP_NOFS);
 	if (!req) {
-		printk_ratelimited(KERN_ERR
-				   "%s: crypto_request_alloc() failed\n",
-				   __func__);
-		ciphertext_page = ERR_PTR(-ENOMEM);
+		res = -ENOMEM;
 		goto out;
 	}
 	ablkcipher_request_set_callback(
 		req, CRYPTO_TFM_REQ_MAY_BACKLOG | CRYPTO_TFM_REQ_MAY_SLEEP,
 		ext4_crypt_complete, &ecr);
-	ext4_xts_tweak_for_page(xts_tweak, plaintext_page);
+	ext4_xts_tweak(xts_tweak, plaintext_page, offset);
 	sg_init_table(&dst, 1);
-	sg_set_page(&dst, ciphertext_page, PAGE_CACHE_SIZE, 0);
+	sg_set_page(&dst, ciphertext_page, size, offset);
 	sg_init_table(&src, 1);
-	sg_set_page(&src, plaintext_page, PAGE_CACHE_SIZE, 0);
-	ablkcipher_request_set_crypt(req, &src, &dst, PAGE_CACHE_SIZE,
-				     xts_tweak);
+	sg_set_page(&src, plaintext_page, size, offset);
+	ablkcipher_request_set_crypt(req, &src, &dst, size, xts_tweak);
 	res = crypto_ablkcipher_encrypt(req);
 	if (res == -EINPROGRESS || res == -EBUSY) {
 		BUG_ON(req->base.data != &ecr);
@@ -417,40 +426,33 @@ static struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
 		res = ecr.res;
 	}
 	ablkcipher_request_free(req);
-	if (res) {
-		printk_ratelimited(
-			KERN_ERR
-			"%s: crypto_ablkcipher_encrypt() returned %d\n",
-			__func__, res);
-		ciphertext_page = ERR_PTR(res);
-		goto out;
-	}
 out:
-	return ciphertext_page;
+	return res;
 }
 
 /**
  * ext4_encrypt() - Encrypts a page
  * @ctx:            The encryption context.
  * @plaintext_page: The page to encrypt. Must be locked.
+ * @offset:         The offset within the page to encrypt.
+ * @size:           The size of the region to encrypt.
  *
  * Allocates a ciphertext page and encrypts plaintext_page into it using the ctx
  * encryption context.
  *
  * Called on the page write path.
  *
- * Return: An allocated page with the encrypted content on success. Else, an
- * error value or NULL.
+ * Return: Zero on success; non-zero otherwise.
  */
-struct page *ext4_encrypt(struct ext4_crypto_ctx *ctx,
-			  struct page *plaintext_page)
+int ext4_encrypt(struct ext4_crypto_ctx *ctx, struct page *plaintext_page,
+		 size_t offset, size_t size)
 {
-	struct page *ciphertext_page = NULL;
+	int res = -EINVAL;
 
 	BUG_ON(!PageLocked(plaintext_page));
 	switch (ctx->mode) {
 	case EXT4_ENCRYPTION_MODE_AES_256_XTS:
-		ciphertext_page = ext4_xts_encrypt(ctx, plaintext_page);
+		res = ext4_xts_encrypt(ctx, plaintext_page, offset, size);
 		break;
 	case EXT4_ENCRYPTION_MODE_AES_256_GCM:
 		/* TODO(mhalcrow): We'll need buffers for the
@@ -458,24 +460,25 @@ struct page *ext4_encrypt(struct ext4_crypto_ctx *ctx,
 		 * ones below */
 	case EXT4_ENCRYPTION_MODE_HMAC_SHA1:
 	case EXT4_ENCRYPTION_MODE_AES_256_XTS_RANDOM_IV_HMAC_SHA1:
-		ciphertext_page = ERR_PTR(-ENOTSUPP);
+		res = -ENOTSUPP;
 		break;
 	default:
 		BUG();
 	}
-	if (!IS_ERR_OR_NULL(ciphertext_page))
-		ext4_prep_pages_for_write(ciphertext_page, plaintext_page, ctx);
-	return ciphertext_page;
+	return res;
 }
 
 /**
  * ext4_xts_decrypt() - Decrypts a page using AES-256-XTS
- * @ctx:  The encryption context.
- * @page: The page to decrypt. Must be locked.
+ * @ctx:    The encryption context.
+ * @page:   The page to decrypt. Must be locked.
+ * @offset: The offset within the page to decrypt.
+ * @size:   The size of the region to decrypt.
  *
  * Return: Zero on success, non-zero otherwise.
  */
-static int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
+static int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page,
+			    size_t offset, size_t size)
 {
 	u8 xts_tweak[EXT4_XTS_TWEAK_SIZE];
 	struct ablkcipher_request *req = NULL;
@@ -503,10 +506,10 @@ static int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
 	ablkcipher_request_set_callback(
 		req, CRYPTO_TFM_REQ_MAY_BACKLOG | CRYPTO_TFM_REQ_MAY_SLEEP,
 		ext4_crypt_complete, &ecr);
-	ext4_xts_tweak_for_page(xts_tweak, page);
+	ext4_xts_tweak(xts_tweak, page, offset);
 	sg_init_table(&sg, 1);
-	sg_set_page(&sg, page, PAGE_CACHE_SIZE, 0);
-	ablkcipher_request_set_crypt(req, &sg, &sg, PAGE_CACHE_SIZE, xts_tweak);
+	sg_set_page(&sg, page, size, offset);
+	ablkcipher_request_set_crypt(req, &sg, &sg, size, xts_tweak);
 	res = crypto_ablkcipher_decrypt(req);
 	if (res == -EINPROGRESS || res == -EBUSY) {
 		BUG_ON(req->base.data != &ecr);
@@ -524,6 +527,8 @@ out:
  * ext4_decrypt() - Decrypts a page in-place
  * @ctx:  The encryption context.
  * @page: The page to decrypt. Must be locked.
+ * @offset:         The offset within the page to decrypt.
+ * @size:           The size of the region to decrypt.
  *
  * Decrypts page in-place using the ctx encryption context.
  *
@@ -531,14 +536,15 @@ out:
  *
  * Return: Zero on success, non-zero otherwise.
  */
-int ext4_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
+int ext4_decrypt(struct ext4_crypto_ctx *ctx, struct page *page, size_t offset,
+		 size_t size)
 {
 	int res = 0;
 
 	BUG_ON(!PageLocked(page));
 	switch (ctx->mode) {
 	case EXT4_ENCRYPTION_MODE_AES_256_XTS:
-		res = ext4_xts_decrypt(ctx, page);
+		res = ext4_xts_decrypt(ctx, page, offset, size);
 		break;
 	case EXT4_ENCRYPTION_MODE_AES_256_GCM:
 	case EXT4_ENCRYPTION_MODE_HMAC_SHA1:
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 292a3a3..4ec0f62 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -2837,11 +2837,15 @@ struct ext4_crypto_ctx *ext4_get_crypto_ctx(
 	bool with_page, const struct ext4_encryption_key *key);
 void ext4_release_crypto_ctx(struct ext4_crypto_ctx *ctx);
 void set_bh_to_page(struct buffer_head *head, struct page *page);
-struct page *ext4_encrypt(struct ext4_crypto_ctx *ctx,
-			  struct page *plaintext_page);
-int ext4_decrypt(struct ext4_crypto_ctx *ctx, struct page *page);
+int ext4_encrypt(struct ext4_crypto_ctx *ctx, struct page *plaintext_page,
+		 size_t offset, size_t size);
+int ext4_decrypt(struct ext4_crypto_ctx *ctx, struct page *page, size_t offset,
+		 size_t size);
 int ext4_get_crypto_key(const struct file *file);
 int ext4_set_crypto_key(struct dentry *dentry);
+void ext4_prep_pages_for_write(struct page *ciphertext_page,
+			       struct page *plaintext_page,
+			       struct ext4_crypto_ctx *ctx);
 static inline bool ext4_is_encryption_enabled(struct ext4_inode_info *ei)
 {
 	return ei->i_encryption_key.mode != EXT4_ENCRYPTION_MODE_INVALID;
diff --git a/fs/ext4/ext4_crypto.h b/fs/ext4/ext4_crypto.h
index 6cb5ba9..e747b51 100644
--- a/fs/ext4/ext4_crypto.h
+++ b/fs/ext4/ext4_crypto.h
@@ -32,12 +32,6 @@
 #define EXT4_MAX_IV_SIZE AES_BLOCK_SIZE
 #define EXT4_MAX_AUTH_SIZE EXT4_AES_256_GCM_AUTH_SIZE
 
-/* The metadata directory is only necessary only for the sibling file
- * directory under the mount root, which will be replaced by per-block
- * metadata when it's ready. */
-#define EXT4_METADATA_DIRECTORY_NAME ".ext4_crypt_data"
-#define EXT4_METADATA_DIRECTORY_NAME_SIZE 16
-
 /**
  * Packet format:
  *  4 bytes: Size of packet (inclusive of these 4 bytes)
@@ -142,6 +136,7 @@ struct ext4_encryption_wrapper_desc {
 
 struct ext4_crypto_ctx {
 	struct crypto_tfm *tfm;         /* Crypto API context */
+	struct mutex tfm_mutex;
 	struct page *bounce_page;	/* Ciphertext page on write path */
 	struct page *control_page;	/* Original page on write path */
 	struct bio *bio;		/* The bio for this context */
@@ -150,6 +145,14 @@ struct ext4_crypto_ctx {
 	int flags;			/* Flags */
 	enum ext4_encryption_mode mode; /* Encryption mode for tfm */
 	atomic_t dbg_refcnt;            /* TODO(mhalcrow): Remove for release */
+	atomic_t refcnt;		/* Number of references to this ctx */
+};
+
+struct ext4_segment_ctx {
+	struct ext4_crypto_ctx *ctx;
+	struct work_struct work;	/* Work queue for read complete path */
+	size_t offset;
+	size_t size;
 };
 
 static inline int ext4_encryption_key_size(enum ext4_encryption_mode mode)
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index bcf8c2e..9f4abc9 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -784,6 +784,7 @@ struct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,
 	struct buffer_head *bh;
 	struct ext4_inode_info *ei = EXT4_I(inode);
 	struct ext4_crypto_ctx *ctx;
+	size_t offset_in_page;
 
 	bh = ext4_getblk(handle, inode, block, create);
 	if (IS_ERR(bh))
@@ -795,7 +796,11 @@ struct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,
 	if (buffer_uptodate(bh)) {
 		if (ext4_is_encryption_enabled(ei)) {
 			ctx = ext4_get_crypto_ctx(false, &ei->i_encryption_key);
-			WARN_ON_ONCE(ext4_decrypt(ctx, bh->b_page));
+			offset_in_page = ((block <<
+					   inode->i_sb->s_blocksize_bits) %
+					  PAGE_CACHE_SIZE);
+			WARN_ON_ONCE(ext4_decrypt(ctx, bh->b_page,
+						  offset_in_page, bh->b_size));
 			ext4_release_crypto_ctx(ctx);
 		}
 		return bh;
@@ -898,7 +903,7 @@ static int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,
 	int err = 0;
 	unsigned blocksize, bbits;
 	struct buffer_head *bh, *head, *wait[2], **wait_bh=wait;
-	bool decrypt = false;
+	size_t offset_in_page;
 	struct ext4_crypto_ctx *ctx;
 
 	BUG_ON(!PageLocked(page));
@@ -953,7 +958,22 @@ static int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,
 		    (block_start < from || block_end > to)) {
 			ll_rw_block(READ, 1, &bh);
 			*wait_bh++=bh;
-			decrypt = ext4_is_encryption_enabled(ei);
+			if (ext4_is_encryption_enabled(ei)) {
+				ctx = ext4_get_crypto_ctx(
+					false, &ei->i_encryption_key);
+				if (!ctx) {
+					err = -ENOMEM;
+					goto out;
+				}
+				offset_in_page = ((block_start * blocksize) %
+						  PAGE_CACHE_SIZE);
+				err = ext4_decrypt(ctx, page, offset_in_page,
+						   blocksize);
+				ext4_release_crypto_ctx(ctx);
+				if (err)
+					break;
+
+			}
 		}
 	}
 	/*
@@ -966,14 +986,6 @@ static int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,
 	}
 	if (unlikely(err)) {
 		page_zero_new_buffers(page, from, to);
-	} else if (decrypt) {
-		ctx = ext4_get_crypto_ctx(false, &ei->i_encryption_key);
-		if (!ctx) {
-			err = -ENOMEM;
-			goto out;
-		}
-		err = ext4_decrypt(ctx, page);
-		ext4_release_crypto_ctx(ctx);
 	}
 out:
 	return err;
@@ -2914,31 +2926,46 @@ static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
 	return generic_block_bmap(mapping, block, ext4_get_block);
 }
 
-static void ext4_completion_work(struct work_struct *work)
+static void ext4_segment_completion_work(struct work_struct *work)
 {
-	struct ext4_crypto_ctx *ctx =
-		container_of(work, struct ext4_crypto_ctx, work);
+	struct ext4_segment_ctx *segment_ctx =
+		container_of(work, struct ext4_segment_ctx, work);
+	struct ext4_crypto_ctx *ctx = segment_ctx->ctx;
 	struct page *page = ctx->control_page;
-	WARN_ON_ONCE(ext4_decrypt(ctx, page));
-	ext4_release_crypto_ctx(ctx);
-	SetPageUptodate(page);
-	unlock_page(page);
+
+	mutex_lock(&ctx->tfm_mutex);
+	WARN_ON_ONCE(ext4_decrypt(ctx, page, segment_ctx->offset,
+				  segment_ctx->size));
+	mutex_unlock(&ctx->tfm_mutex);
+	if (atomic_dec_and_test(&ctx->refcnt)) {
+		SetPageUptodate(page);
+		unlock_page(page);
+		ext4_release_crypto_ctx(ctx);
+	}
+	kfree(segment_ctx);
 }
 
-static int ext4_complete_cb(struct bio *bio, int res)
+static int ext4_complete_segment_cb(struct bio *bio, int res)
 {
-	struct ext4_crypto_ctx *ctx = bio->bi_cb_ctx;
-	struct page *page = ctx->control_page;
+	struct ext4_segment_ctx *segment_ctx = bio->bi_cb_ctx;
+
 	if (res) {
-		ext4_release_crypto_ctx(ctx);
-		unlock_page(page);
+		if (atomic_dec_and_test(&segment_ctx->ctx->refcnt))
+			ext4_release_crypto_ctx(segment_ctx->ctx);
+		kfree(segment_ctx);
 		return res;
 	}
-	INIT_WORK(&ctx->work, ext4_completion_work);
-	queue_work(mpage_read_workqueue, &ctx->work);
+	BUG_ON(!segment_ctx);
+	INIT_WORK(&segment_ctx->work, ext4_segment_completion_work);
+	queue_work(mpage_read_workqueue, &segment_ctx->work);
 	return 0;
 }
 
+struct bh_to_submit {
+	struct buffer_head *bh;
+	size_t offset;
+};
+
 static int ext4_read_full_page(struct page *page)
 {
 	struct inode *inode = page->mapping->host;
@@ -2948,10 +2975,12 @@ static int ext4_read_full_page(struct page *page)
 	sector_t iblock = (sector_t)page->index << (PAGE_CACHE_SHIFT - bbits);
 	sector_t lblock = (i_size_read(inode)+blocksize-1) >> bbits;
 	struct buffer_head *bh = head;
-	struct buffer_head *arr[MAX_BUF_PER_PAGE];
+	struct bh_to_submit arr[MAX_BUF_PER_PAGE];
+	struct ext4_crypto_ctx *ctx = NULL;
 	int nr = 0;
 	int i = 0;
 	int fully_mapped = 1;
+	size_t offset;
 
 	do {
 		if (buffer_uptodate(bh))
@@ -2980,7 +3009,8 @@ static int ext4_read_full_page(struct page *page)
 			if (buffer_uptodate(bh))
 				continue;
 		}
-		arr[nr++] = bh;
+		arr[nr].bh = bh;
+		arr[nr++].offset = i * bh->b_size;
 	} while (i++, iblock++, (bh = bh->b_this_page) != head);
 
 	if (fully_mapped)
@@ -2997,15 +3027,9 @@ static int ext4_read_full_page(struct page *page)
 		return 0;
 	}
 
-	/*
-	 * Encryption requires blocksize is page size, so we should never see
-	 * more than one buffer head per page.
-	 */
-	BUG_ON(nr != 1);
-
 	/* Stage two: lock the buffers */
 	for (i = 0; i < nr; i++) {
-		bh = arr[i];
+		bh = arr[i].bh;
 		lock_buffer(bh);
 		mark_buffer_async_read(bh);
 	}
@@ -3015,18 +3039,48 @@ static int ext4_read_full_page(struct page *page)
 	 * inside the buffer lock in case another process reading
 	 * the underlying blockdev brought it uptodate (the sct fix).
 	 */
-	for (i = 0; i < nr; i++) {
-		bh = arr[i];
-		if (buffer_uptodate(bh))
+	for (i = 0; i < nr; i++, offset += head->b_size) {
+		bh = arr[i].bh;
+		offset = arr[i].offset;
+		if (buffer_uptodate(bh)) {
 			end_buffer_async_read(bh, 1);
-		else {
+		} else {
 			struct ext4_inode_info *ei = EXT4_I(inode);
-			struct ext4_crypto_ctx *ctx = ext4_get_crypto_ctx(
-				false, &ei->i_encryption_key);
-			atomic_inc(&ctx->dbg_refcnt);
-			ctx->control_page = page;
-			if (submit_bh_cb(READ, bh, ext4_complete_cb, ctx))
-				ext4_release_crypto_ctx(ctx);
+			struct ext4_segment_ctx *segment_ctx;
+
+			if (!ctx) {
+				ctx = ext4_get_crypto_ctx(
+					false, &ei->i_encryption_key);
+				if (!ctx)
+					break;
+				atomic_inc(&ctx->dbg_refcnt);
+				ctx->control_page = page;
+
+				/* Keep ref until all segments are submitted */
+				atomic_inc(&ctx->refcnt);
+			}
+			segment_ctx = kzalloc(sizeof(struct ext4_segment_ctx),
+					      GFP_NOFS);
+			if (!segment_ctx)
+				continue;
+			segment_ctx->offset = offset;
+			segment_ctx->size = head->b_size;
+			segment_ctx->ctx = ctx;
+			atomic_inc(&ctx->refcnt);
+			if (submit_bh_cb(READ, bh,
+					 ext4_complete_segment_cb,
+					 segment_ctx)) {
+				atomic_dec(&ctx->refcnt);
+				kfree(segment_ctx);
+				continue;
+			}
+		}
+	}
+	if (ctx) {
+		if (atomic_dec_and_test(&ctx->refcnt)) {
+			SetPageUptodate(page);
+			unlock_page(page);
+			ext4_release_crypto_ctx(ctx);
 		}
 	}
 	return 0;
@@ -3477,6 +3531,7 @@ static int ext4_block_zero_page_range(handle_t *handle,
 	struct buffer_head *bh;
 	struct page *page;
 	struct ext4_crypto_ctx *ctx;
+	size_t offset_in_page;
 	int err = 0;
 
 	page = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,
@@ -3533,9 +3588,12 @@ static int ext4_block_zero_page_range(handle_t *handle,
 		if (!buffer_uptodate(bh))
 			goto unlock;
 		if (ext4_is_encryption_enabled(ei)) {
-			BUG_ON(blocksize != PAGE_CACHE_SIZE);
 			ctx = ext4_get_crypto_ctx(false, &ei->i_encryption_key);
-			WARN_ON_ONCE(ext4_decrypt(ctx, page));
+			offset_in_page = ((iblock <<
+					   inode->i_sb->s_blocksize_bits) %
+					  PAGE_CACHE_SIZE);
+			WARN_ON_ONCE(ext4_decrypt(ctx, page, offset_in_page,
+						  bh->b_size));
 			ext4_release_crypto_ctx(ctx);
 		}
 	}
diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index b68d178..8109eba08 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -434,57 +434,26 @@ static void ext4_abort_bio_write(struct page *page,
 	} while (bh != head);
 }
 
-static int io_encrypt_submit_page(struct ext4_io_submit *io, struct page *page)
-{
-	struct page *data_page = NULL;
-	struct ext4_crypto_ctx *ctx = NULL;
-	struct inode *inode = page->mapping->host;
-	struct ext4_inode_info *ei = EXT4_I(inode);
-	struct buffer_head *bh;
-	int res = 0;
-
-	ctx = ext4_get_crypto_ctx(true, &ei->i_encryption_key);
-	if (IS_ERR(ctx))
-		return PTR_ERR(ctx);
-
-	bh = page_buffers(page);
-	data_page = ext4_encrypt(ctx, page);
-	if (IS_ERR(data_page)) {
-		ext4_release_crypto_ctx(ctx);
-		res = PTR_ERR(data_page);
-		printk_ratelimited(KERN_ERR "%s: ext4_encrypt() returned %d\n",
-				   __func__, res);
-		goto out;
-	}
-	lock_page(data_page);
-	res = io_submit_add_bh(io, inode, bh);
-	if (res)
-		ext4_restore_control_page(data_page);
-out:
-	return res;
-}
-
-static int ext4_bio_write_buffers(struct ext4_io_submit *io,
-				  struct page *page,
-				  int len,
-				  struct writeback_control *wbc)
+/**
+ * ext4_prep_bh() - Prepare and mark buffers to submit
+ * @io:
+ * @page:
+ *
+ * We have to mark all buffers in the page before submitting so that
+ * end_page_writeback() cannot be called from ext4_bio_end_io() when IO on the
+ * first buffer finishes and we are still working on submitting the second
+ * buffer.
+ */
+static void ext4_prep_bh(struct ext4_io_submit *io, struct page *page, int len)
 {
-	struct inode *inode = page->mapping->host;
-	struct ext4_inode_info *ei = EXT4_I(inode);
 	unsigned block_start;
 	struct buffer_head *bh, *head;
-	int ret = 0;
-	int nr_submitted = 0;
 
-	/*
-	 * In the first loop we prepare and mark buffers to submit. We have to
-	 * mark all buffers in the page before submitting so that
-	 * end_page_writeback() cannot be called from ext4_bio_end_io() when IO
-	 * on the first buffer finishes and we are still working on submitting
-	 * the second buffer.
-	 */
+	size_t offset = 0;
+
 	bh = head = page_buffers(page);
 	do {
+		offset += head->b_size;
 		block_start = bh_offset(bh);
 		if (block_start >= len) {
 			clear_buffer_dirty(bh);
@@ -505,17 +474,79 @@ static int ext4_bio_write_buffers(struct ext4_io_submit *io,
 		}
 		set_buffer_async_write(bh);
 	} while ((bh = bh->b_this_page) != head);
+}
+
+static struct ext4_crypto_ctx *ext4_encrypt_bh(struct ext4_inode_info *ei,
+					       struct page *page)
+{
+	struct ext4_crypto_ctx *ctx = NULL;
+	size_t offset, next_offset = 0;
+	struct buffer_head *bh, *head;
+	int ret;
+
+	bh = head = page_buffers(page);
+	do {
+		offset = next_offset;
+		next_offset += head->b_size;
+		if (!buffer_async_write(bh))
+			continue;
+		if (!ctx) {
+			ctx = ext4_get_crypto_ctx(true, &ei->i_encryption_key);
+			if (IS_ERR(ctx))
+				goto out;
+		}
+		ret = ext4_encrypt(ctx, page, offset, head->b_size);
+		if (ret)
+			goto out;
+	} while ((bh = bh->b_this_page) != head);
+out:
+	if (ret) {
+		if (ctx)
+			ext4_release_crypto_ctx(ctx);
+		ctx = ERR_PTR(ret);
+	}
+	return ctx;
+}
+
+struct inode *ext4_inode;
+EXPORT_SYMBOL(ext4_inode);
+
+static int ext4_bio_write_buffers(struct ext4_io_submit *io,
+				  struct page *page, int len,
+				  struct writeback_control *wbc)
+{
+	struct inode *inode = page->mapping->host;
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	struct buffer_head *bh, *head;
+	struct ext4_crypto_ctx *ctx = NULL;
+	bool pages_prepped = false;
+	int ret = 0;
+	int nr_submitted = 0;
+
+	/* Mark buffer heads for write */
+	ext4_prep_bh(io, page, len);
+
+	/* Encrypt the buffers if encryption is enabled */
+	if (ext4_is_encryption_enabled(ei)) {
+		ctx = ext4_encrypt_bh(ei, page);
+		if (IS_ERR(ctx)) {
+			ret = PTR_ERR(ctx);
+			ctx = NULL;
+			goto out;
+		}
+	}
 
 	/* Now submit buffers to write */
 	bh = head = page_buffers(page);
 	do {
 		if (!buffer_async_write(bh))
 			continue;
-		if (ext4_is_encryption_enabled(ei)) {
-			ret = io_encrypt_submit_page(io, page);
-		} else {
-			ret = io_submit_add_bh(io, inode, bh);
+		if (ctx && !pages_prepped) {
+			ext4_prep_pages_for_write(ctx->bounce_page, page, ctx);
+			/* ctx->bounce_page now owns its ctx */
+			pages_prepped = true;
 		}
+		ret = io_submit_add_bh(io, inode, bh);
 		if (ret) {
 			/*
 			 * We only get here on ENOMEM.  Not much else
@@ -528,15 +559,25 @@ static int ext4_bio_write_buffers(struct ext4_io_submit *io,
 		clear_buffer_dirty(bh);
 	} while ((bh = bh->b_this_page) != head);
 
+out:
 	/* Error stopped previous loop? Clean up buffers... */
 	if (ret) {
 		printk_ratelimited(KERN_ERR "%s: ret = %d\n", __func__, ret);
+		if (ctx) {
+			if (pages_prepped)
+				ext4_restore_control_page(ctx->bounce_page);
+			else
+				ext4_release_crypto_ctx(ctx);
+		}
 		ext4_abort_bio_write(page, wbc);
 	}
 	unlock_page(page);
 	/* Nothing submitted - we have to end page writeback */
-	if (!nr_submitted)
+	if (!nr_submitted) {
+		if (ctx)
+			ext4_release_crypto_ctx(ctx);
 		end_page_writeback(page);
+	}
 	return ret;
 }
 
-- 
2.2.0.rc0.207.ga3a616c


