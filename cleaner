Introduce cleaner

From: Abutalib Aghayev <agayev@cs.cmu.edu>

An experimental cleaner.  Copy the live blocks from the transaction at the
tail in batches to the transaction at the head.  After a commit ends, check
if free space is below watermark and start cleaning until free space is
above high watermark.

Signed-off-by: Abutalib Aghayev <agayev@cs.cmu.edu>
Signed-off-by: Theodore Ts'o <tytso@mit.edu>
---
 fs/jbd2/Makefile     |   2 +-
 fs/jbd2/checkpoint.c |   3 +
 fs/jbd2/cleaner.c    | 369 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 fs/jbd2/jmap.c       |  32 +++++++
 fs/jbd2/journal.c    |  12 +++
 include/linux/jbd2.h |   6 +-
 include/linux/jmap.h |  82 ++++++++++++++++++
 7 files changed, 504 insertions(+), 2 deletions(-)

diff --git a/fs/jbd2/Makefile b/fs/jbd2/Makefile
index a54f50b3a06e..b6a2dddcc0a7 100644
--- a/fs/jbd2/Makefile
+++ b/fs/jbd2/Makefile
@@ -5,4 +5,4 @@
 obj-$(CONFIG_JBD2) += jbd2.o
 
 jbd2-objs := transaction.o commit.o recovery.o checkpoint.o revoke.o journal.o \
-		jmap.o
+		jmap.o cleaner.o
diff --git a/fs/jbd2/checkpoint.c b/fs/jbd2/checkpoint.c
index 4055f51617ef..b60bbf58e8f7 100644
--- a/fs/jbd2/checkpoint.c
+++ b/fs/jbd2/checkpoint.c
@@ -389,6 +389,9 @@ int jbd2_cleanup_journal_tail(journal_t *journal)
 	tid_t		first_tid;
 	unsigned long	blocknr;
 
+	if (journal->j_flags & JBD2_LAZY)
+		return 0;
+
 	if (is_journal_aborted(journal))
 		return -EIO;
 
diff --git a/fs/jbd2/cleaner.c b/fs/jbd2/cleaner.c
new file mode 100644
index 000000000000..163557f726a8
--- /dev/null
+++ b/fs/jbd2/cleaner.c
@@ -0,0 +1,369 @@
+#include <linux/blk_types.h>
+#include <linux/jbd2.h>
+#include <linux/jmap.h>
+#include <linux/list.h>
+#include <linux/blkdev.h>
+#include <linux/completion.h>
+#include <linux/delay.h>
+#include <trace/events/jbd2.h>
+
+inline int jbd2_low_on_space(journal_t *journal)
+{
+	int x = atomic_read(&journal->j_cleaner_ctx->nr_txns_committed);
+	if (x > 10) {
+		trace_jbd2_jmap_printf1("low on space", x);
+		return true;
+	}
+	trace_jbd2_jmap_printf1("not low on space", x);
+	return false;
+}
+
+inline int jbd2_high_on_space(journal_t *journal)
+{
+	if (atomic_read(&journal->j_cleaner_ctx->nr_txns_cleaned) < 2) {
+		trace_jbd2_jmap_printf("not enough cleaned");
+		return false;
+	}
+	trace_jbd2_jmap_printf("enough cleaned");
+	atomic_set(&journal->j_cleaner_ctx->nr_txns_cleaned, 0);
+	atomic_set(&journal->j_cleaner_ctx->nr_txns_committed, 0);
+	return true;
+}
+
+inline bool jbd2_cleaning(journal_t *journal)
+{
+	return atomic_read(&journal->j_cleaner_ctx->cleaning);
+}
+
+inline void jbd2_stop_cleaning(journal_t *journal)
+{
+	trace_jbd2_jmap_printf("stopped cleaning");
+	atomic_set(&journal->j_cleaner_ctx->cleaning, 0);
+}
+
+inline void jbd2_start_cleaning(journal_t *journal)
+{
+	struct cleaner_ctx *ctx = journal->j_cleaner_ctx;
+
+	trace_jbd2_jmap_printf("started cleaning");
+	atomic_set(&journal->j_cleaner_ctx->cleaning, 1);
+
+	/* Schedule the next batch of cleaning */
+	if (!jbd2_cleaning_batch_complete(journal)) {
+		trace_jbd2_jmap_printf("not scheduling a new batch");
+		return;
+	}
+
+	trace_jbd2_jmap_printf("scheduling a batch");
+	BUG_ON(atomic_read(&ctx->nr_pending_reads));
+
+	atomic_set(&ctx->batch_in_progress, 1);
+	schedule_work(&ctx->work);
+
+
+}
+
+inline bool jbd2_cleaning_batch_complete(journal_t *journal)
+{
+	return jbd2_cleaning(journal) &&
+		atomic_read(&journal->j_cleaner_ctx->batch_in_progress) == 0;
+}
+
+/*
+ * Tries to move the tail forward (hence free space) as long as the transaction
+ * at the tail has only stale blocks.  Returns true if manages to free a
+ * transaction, false otherwise.
+ */
+bool jbd2_try_to_move_tail(journal_t *journal)
+{
+	struct transaction_infos *tis = journal->j_transaction_infos;
+	struct transaction_info *ti, *ti1;
+
+	/*
+	 * Advance the tail as far as possible by skipping over transactions
+	 * with no live blocks.
+	 */
+	write_lock(&journal->j_jmap_lock);
+	ti = ti1 = &tis->buf[tis->tail];
+
+	for ( ; list_empty(&ti->live_blks); ti = &tis->buf[tis->tail]) {
+		trace_jbd2_jmap_printf2("cleaned a transaction",
+					tis->tail, ti->tid);
+		tis->tail = (tis->tail + 1) & (MAX_LIVE_TRANSACTIONS - 1);
+		atomic_inc(&journal->j_cleaner_ctx->nr_txns_cleaned);
+	}
+	write_unlock(&journal->j_jmap_lock);
+
+	if (ti == ti1)
+		return false;
+	/*
+	 * In the worst case, this will end up updating the journal superblock
+	 * after cleaning up every transaction.  Should we avoid it?
+	 */
+	write_unlock(&journal->j_state_lock);
+	jbd2_update_log_tail(journal, ti->tid, ti->offset);
+	write_lock(&journal->j_state_lock);
+
+	return true;
+}
+
+/*
+ * Finds the live blocks at the tail transaction and copies the corresponding
+ * mappings to |ctx->mappings|.  Returns the number of live block mappings
+ * copied.  Should be called with a read lock on |j_jmap_lock|.
+ */
+static int find_live_blocks(struct cleaner_ctx *ctx)
+{
+	journal_t *journal = ctx->journal;
+	struct transaction_infos *tis = journal->j_transaction_infos;
+	struct transaction_info *ti = &tis->buf[tis->tail];
+	struct jmap_entry *je = NULL;
+	int i, nr_live = 0;
+
+	if (unlikely(list_empty(&ti->live_blks)))
+		goto done;
+
+	spin_lock(&ctx->pos_lock);
+	if (!ctx->pos)
+		ctx->pos = list_first_entry(&ti->live_blks, typeof(*je), list);
+	je = ctx->pos;
+	spin_unlock(&ctx->pos_lock);
+
+	list_for_each_entry_from(je, &ti->live_blks, list) {
+		if (je->revoked)
+			continue;
+		ctx->mappings[nr_live++] = je->mapping;
+		if (nr_live == CLEANER_BATCH_SIZE)
+			break;
+	}
+
+done:
+	trace_jbd2_jmap_printf1("found live blocks", nr_live);
+	for (i = 0; i < nr_live; ++i)
+		trace_jbd2_jmap_printf2("m",
+					ctx->mappings[i].fsblk,
+					ctx->mappings[i].logblk);
+	return nr_live;
+}
+
+static void live_block_read_end_io(struct buffer_head *bh, int uptodate)
+{
+	struct cleaner_ctx *ctx = bh->b_private;
+
+	if (uptodate) {
+		set_buffer_uptodate(bh);
+		if (atomic_dec_and_test(&ctx->nr_pending_reads))
+			complete(&ctx->live_block_reads);
+	} else {
+		WARN_ON(1);
+		clear_buffer_uptodate(bh);
+	}
+
+	unlock_buffer(bh);
+	put_bh(bh);
+}
+
+/*
+ * Reads live blocks in |ctx->mappings| populated by find_live_blocks into
+ * buffer heads in |ctx->bhs|.  Returns true if at least one of the reads goes
+ * out to disk and false otherwise.  If this function returns true then the
+ * client should sleep on the condition variable |ctx->live_block_reads|.  The
+ * client will be woken up when all reads are complete, through the end_io
+ * handler attached to buffer heads read from disk.
+ */
+static bool read_live_blocks(struct cleaner_ctx *ctx, int nr_live)
+{
+	journal_t *journal = ctx->journal;
+	bool slow = false;
+	struct blk_plug plug;
+	bool plugged = false;
+	int i, rc;
+
+	for (i = 0; i < nr_live; ++i) {
+		ctx->bhs[i] = __getblk(journal->j_dev, ctx->mappings[i].fsblk,
+				journal->j_blocksize);
+		if (unlikely(!ctx->bhs[i]))
+			goto out_err;
+		if (buffer_uptodate(ctx->bhs[i]))
+			continue;
+		plugged = true;
+		blk_start_plug(&plug);
+		lock_buffer(ctx->bhs[i]);
+		ctx->bhs[i]->b_private = ctx;
+		ctx->bhs[i]->b_end_io = live_block_read_end_io;
+		atomic_inc(&ctx->nr_pending_reads);
+		get_bh(ctx->bhs[i]);
+		rc = read_block_from_log(ctx->journal, ctx->bhs[i],
+					 REQ_RAHEAD, ctx->mappings[i].logblk);
+		if (unlikely(rc < 0))
+			goto out_err;
+		if (rc) {
+			slow = true;
+			trace_jbd2_jmap_printf2("reading from disk",
+						ctx->mappings[i].fsblk,
+						ctx->mappings[i].logblk);
+		} else {
+			trace_jbd2_jmap_printf2("cached",
+						ctx->mappings[i].fsblk,
+						ctx->mappings[i].logblk);
+		}
+	}
+	if (plugged)
+		blk_finish_plug(&plug);
+	return slow;
+
+out_err:
+	jbd2_journal_abort(ctx->journal, -ENOMEM);
+	return false;
+}
+
+/*
+ * This function finds the live blocks that became stale between the call to
+ * find_live_blocks and now, and discards them.  It returns true if there are no
+ * more live blocks left at the tail transaction.
+ */
+static bool discard_stale_blocks(struct cleaner_ctx *ctx, int nr_live)
+{
+	journal_t *journal = ctx->journal;
+	struct transaction_infos *tis = journal->j_transaction_infos;
+	struct transaction_info *ti = &tis->buf[tis->tail];
+	struct jmap_entry *je = NULL;
+	int i = 0, j = 0, next = 0;
+
+	trace_jbd2_jmap_printf(__func__);
+	spin_lock(&ctx->pos_lock);
+	BUG_ON(!ctx->pos);
+	je = ctx->pos;
+	list_for_each_entry_from(je, &ti->live_blks, list) {
+		for (j = next; j < nr_live; ++j) {
+			if (je->mapping.fsblk == ctx->mappings[j].fsblk) {
+				next = j+1;
+				ctx->pos = list_next_entry(je, list);
+				if (je->revoked) {
+					brelse(ctx->bhs[j]);
+					ctx->bhs[j] = NULL;
+					trace_jbd2_jmap_printf2(
+						"revoked",
+						ctx->mappings[i].fsblk,
+						ctx->mappings[i].logblk);
+				}
+				break;
+			} else {
+				trace_jbd2_jmap_printf2(
+						"moved to another list",
+						ctx->mappings[i].fsblk,
+						ctx->mappings[i].logblk);
+				brelse(ctx->bhs[j]);
+				ctx->bhs[j] = NULL;
+			}
+		}
+		if (++i == nr_live || j == nr_live)
+			break;
+	}
+	spin_unlock(&ctx->pos_lock);
+
+	/*
+	 * We have exited the loop.  If we haven't processed all the entries in
+	 * |ctx->mappings|, that is if (j < nr_live) at the exit, and we have
+	 * not processed |nr_live| entries from the live blocks list at the
+	 * tail, that is if (i < nr_live) at the exit, then the live blocks list
+	 * has shrunk and the tail transaction has no live blocks left.
+	 */
+	return j < nr_live && i < nr_live;
+}
+
+static void attach_live_blocks(struct cleaner_ctx *ctx, handle_t *handle,
+			       int nr_live)
+{
+	int err, i;
+
+	trace_jbd2_jmap_printf(__func__);
+	for (i = 0; i < nr_live; ++i) {
+		if (!ctx->bhs[i])
+			continue;
+		trace_jbd2_jmap_printf2("attaching",
+					ctx->mappings[i].fsblk,
+					ctx->mappings[i].logblk);
+		err = jbd2_journal_get_write_access(handle, ctx->bhs[i]);
+		if (!err)
+			err = jbd2_journal_dirty_metadata(handle, ctx->bhs[i]);
+		if (err) {
+			jbd2_journal_abort(ctx->journal, err);
+			return;
+		}
+	}
+}
+
+/*
+ * Read the live blocks from the tail transaction and attach them to the current
+ * transaction.
+ */
+void jbd2_jmap_do_clean_batch(struct work_struct *work)
+{
+	struct cleaner_ctx *ctx = container_of(work, struct cleaner_ctx, work);
+	bool wake_up_commit_thread = true;
+	handle_t *handle = NULL;
+	int nr_live, err;
+
+	read_lock(&ctx->journal->j_jmap_lock);
+	nr_live = find_live_blocks(ctx);
+	read_unlock(&ctx->journal->j_jmap_lock);
+
+	if (nr_live < CLEANER_BATCH_SIZE)
+		wake_up_commit_thread = false;
+	if (nr_live == 0)
+		goto done;
+
+	reinit_completion(&ctx->live_block_reads);
+	if (read_live_blocks(ctx, nr_live)) {
+		trace_jbd2_jmap_printf("waiting for completion");
+		wait_for_completion(&ctx->live_block_reads);
+	} else {
+		trace_jbd2_jmap_printf("not waiting for completion");
+	}
+	if (atomic_read(&ctx->nr_pending_reads)) {
+		/* Should never trigger, but we'll do better
+		 * converting to a wait_channel instead of the compeltion */
+		pr_err("JBD2: clean_batch: completion failed, recovering\n");
+		mdelay(100);
+	}
+
+	handle = jbd2_journal_start(ctx->journal, nr_live);
+	if (IS_ERR(handle)) {
+		jbd2_journal_abort(ctx->journal, PTR_ERR(handle));
+		return;
+	}
+
+	read_lock(&ctx->journal->j_jmap_lock);
+	if (discard_stale_blocks(ctx, nr_live))
+		wake_up_commit_thread = false;
+	read_unlock(&ctx->journal->j_jmap_lock);
+	/*
+	 * I'm not sure why this function was under the jmap_lock
+	 * previously, but it can't be, since it calls functions that
+	 * can block due to memory allocation.  I don't think it needs
+	 * to be protected, since it appears that ctx->mapping is only
+	 * used by the cleaner code, and so it can't be run multiple
+	 * times.  -- TYT
+	 */
+	attach_live_blocks(ctx, handle, nr_live);
+
+	err = jbd2_journal_stop(handle);
+	if (err) {
+		jbd2_journal_abort(ctx->journal, err);
+		return;
+	}
+
+done:
+	atomic_set(&ctx->batch_in_progress, 0);
+	atomic_inc(&ctx->nr_txns_cleaned);
+	if (wake_up_commit_thread) {
+		trace_jbd2_jmap_printf("waking up commit thread");
+		wake_up(&ctx->journal->j_wait_commit);
+	} else {
+		trace_jbd2_jmap_printf("not waking up commit thread");
+		spin_lock(&ctx->pos_lock);
+		ctx->pos = NULL;
+		spin_unlock(&ctx->pos_lock);
+	}
+}
diff --git a/fs/jbd2/jmap.c b/fs/jbd2/jmap.c
index 18dd0e127aff..9b317608f70a 100644
--- a/fs/jbd2/jmap.c
+++ b/fs/jbd2/jmap.c
@@ -91,8 +91,17 @@ static int process_existing_mappings(journal_t *journal,
 		 * We are either deleting the entry because it was revoked, or
 		 * we are moving it to the live blocks list of this transaction.
 		 * In either case, we remove it from its existing list.
+		 * However, before removing it we check to see if this is an
+		 * entry in the live blocks list of the tail transaction a
+		 * pointer to whom is cached by the cleaner and update the
+		 * cached pointer if so.
 		 */
+		spin_lock(&journal->j_cleaner_ctx->pos_lock);
+		if (je == journal->j_cleaner_ctx->pos) {
+			journal->j_cleaner_ctx->pos = list_next_entry(je, list);
+		}
 		list_del(&je->list);
+		spin_unlock(&journal->j_cleaner_ctx->pos_lock);
 
 		if (je->revoked) {
 			rb_erase(&je->rb_node, &journal->j_jmap);
@@ -245,6 +254,8 @@ int jbd2_transaction_infos_add(journal_t *journal, transaction_t *transaction,
 	 */
 	BUG_ON(!list_empty(&ti->live_blks));
 
+	atomic_inc(&journal->j_cleaner_ctx->nr_txns_committed);
+
 	write_lock(&journal->j_jmap_lock);
 	nr_new = process_existing_mappings(journal, ti, t_idx, mappings,
 					nr_mappings);
@@ -489,11 +500,32 @@ int jbd2_smr_journal_init(journal_t *journal)
 {
 	journal->j_jmap = RB_ROOT;
 	rwlock_init(&journal->j_jmap_lock);
+	journal->j_cleaner_ctx = kzalloc(sizeof(struct cleaner_ctx),
+					 GFP_KERNEL);
+	if (!journal->j_cleaner_ctx)
+		return -ENOMEM;
+
+	journal->j_cleaner_ctx->journal = journal;
+	journal->j_cleaner_ctx->pos = NULL;
+	spin_lock_init(&journal->j_cleaner_ctx->pos_lock);
+	atomic_set(&journal->j_cleaner_ctx->cleaning, 0);
+	atomic_set(&journal->j_cleaner_ctx->batch_in_progress, 0);
+	atomic_set(&journal->j_cleaner_ctx->nr_pending_reads, 0);
+	atomic_set(&journal->j_cleaner_ctx->nr_txns_committed, 0);
+	atomic_set(&journal->j_cleaner_ctx->nr_txns_cleaned, 0);
+	init_completion(&journal->j_cleaner_ctx->live_block_reads);
+	INIT_WORK(&journal->j_cleaner_ctx->work, jbd2_jmap_do_clean_batch);
 	return jbd2_init_transaction_infos(journal);
 }
 
 void jbd2_smr_journal_exit(journal_t *journal)
 {
+	if (journal->j_cleaner_ctx) {
+		atomic_set(&journal->j_cleaner_ctx->cleaning, 0);
+		flush_work(&journal->j_cleaner_ctx->work);
+		kfree(journal->j_cleaner_ctx);
+		journal->j_cleaner_ctx = NULL;
+	}
 	jbd2_free_transaction_infos(journal);
 }
 
diff --git a/fs/jbd2/journal.c b/fs/jbd2/journal.c
index 493b72c60335..ab7b9bbc9296 100644
--- a/fs/jbd2/journal.c
+++ b/fs/jbd2/journal.c
@@ -227,6 +227,15 @@ static int kjournald2(void *arg)
 	}
 
 	wake_up(&journal->j_wait_done_commit);
+
+	if ((journal->j_flags & JBD2_LAZY) &&
+	    (jbd2_cleaning(journal) || jbd2_low_on_space(journal))) {
+		if (jbd2_try_to_move_tail(journal) && jbd2_high_on_space(journal))
+			jbd2_stop_cleaning(journal);
+		else
+			jbd2_start_cleaning(journal);
+	}
+
 	if (freezing(current)) {
 		/*
 		 * The simpler the better. Flushing journal isn't a
@@ -255,6 +264,9 @@ static int kjournald2(void *arg)
 			should_sleep = 0;
 		if (journal->j_flags & JBD2_UNMOUNT)
 			should_sleep = 0;
+		if ((journal->j_flags & JBD2_LAZY) &&
+		    jbd2_cleaning_batch_complete(journal))
+			should_sleep = 0;
 		if (should_sleep) {
 			write_unlock(&journal->j_state_lock);
 			schedule();
diff --git a/include/linux/jbd2.h b/include/linux/jbd2.h
index 771588026353..3112fba26598 100644
--- a/include/linux/jbd2.h
+++ b/include/linux/jbd2.h
@@ -735,7 +735,8 @@ jbd2_time_diff(unsigned long start, unsigned long end)
  * @j_superblock: Second part of superblock buffer
  * @j_map: A map from file system blocks to log blocks
  * @j_transaction_infos: An array of information structures per live transaction
- * @j_map_lock: Protect j_jmap and j_transaction_infos
+ * @j_jmap_lock: Protect j_jmap and j_transaction_infos
+ * @j_cleaner_ctx: Cleaner state
  * @j_format_version: Version of the superblock format
  * @j_state_lock: Protect the various scalars in the journal
  * @j_barrier_count:  Number of processes waiting to create a barrier lock
@@ -820,6 +821,9 @@ struct journal_s
 	/* Protect j_jmap and j_transaction_infos */
 	rwlock_t		j_jmap_lock;
 
+	/* Cleaner state */
+	struct cleaner_ctx	*j_cleaner_ctx;
+
 	/* Version of the superblock format */
 	int			j_format_version;
 
diff --git a/include/linux/jmap.h b/include/linux/jmap.h
index 638f25df8302..5af1fec4ab95 100644
--- a/include/linux/jmap.h
+++ b/include/linux/jmap.h
@@ -132,5 +132,87 @@ extern int jbd2_bh_submit_read(journal_t *journal, struct buffer_head *bh,
 			       const char *func);
 extern void jbd2_sb_breadahead(journal_t *journal, struct super_block *sb,
 			       sector_t block);
+extern void jbd2_jmap_do_clean_batch(struct work_struct *work);
+
+/*
+ * Cleaner stuff is below.
+ */
+
+/*
+ * Number of blocks to read at once, for cleaning.
+ */
+#define CLEANER_BATCH_SIZE 16
+
+/*
+ * Context structure for the cleaner.
+ */
+struct cleaner_ctx {
+	/*
+	 * We set to true once we drop below low watermark and it stays so until
+	 * we rise above the high watermark.  It is accessed by the commit
+	 * thread and the foreground kernel threads during the journal
+	 * destruction, therefore it is atomic.
+	 */
+	atomic_t cleaning;
+
+	/*
+	 * We clean in batches of blocks.  This flag indicates if we are
+	 * currently cleaning a batch.  It is accessed by the commit thread and
+	 * the cleaner thread, therefore it is atomic.
+	 */
+	atomic_t batch_in_progress;
+
+	/*
+	 * We find live blocks to clean from the live blocks list of the
+	 * transaction at the tail.  This list can be larger than our batch size
+	 * and we may need several attempts to process it.  We cache the
+	 * position of the next entry to start from in |pos|.  Since cleaner
+	 * thread can run concurrently with the commit thread that can modify
+	 * the live blocks list of the transaction at the tail (for example, if
+	 * it needs to drop a revoked entry or if |pos| points to an entry that
+	 * has been updated and should move from the live blocks list of the
+	 * transaction at the tail to the live blocks list of current
+	 * transaction) we protect |pos| with |pos_lock|.
+	 */
+	struct jmap_entry *pos;
+	spinlock_t pos_lock;
+
+	/*
+	 * Live block mappings for the blocks that we copy in a batch.
+	 */
+	struct blk_mapping mappings[CLEANER_BATCH_SIZE];
+
+	/*
+	 * Buffer heads for the live blocks read in a batch.
+	 */
+	struct buffer_head *bhs[CLEANER_BATCH_SIZE];
+
+	/*
+	 * Number of pending reads in a batch.  Every submitted read increments
+	 * it and every completed read decrements it.
+	 */
+	atomic_t nr_pending_reads;
+
+	/*
+	 * The cleaner thread sleeps on this condition variable until the last
+	 * completed read wakes the up the cleaner thread.
+	 */
+	struct completion live_block_reads;
+
+	/* TODO: temporary for debugging, remove once done. */
+	atomic_t nr_txns_committed;
+	atomic_t nr_txns_cleaned;
+
+	journal_t *journal;
+	struct work_struct work;
+};
+
+extern int jbd2_low_on_space(journal_t *journal);
+extern int jbd2_high_on_space(journal_t *journal);
+extern bool jbd2_cleaning(journal_t *journal);
+extern void jbd2_stop_cleaning(journal_t *journal);
+extern void jbd2_start_cleaning(journal_t *journal);
+extern bool jbd2_cleaning_batch_complete(journal_t *journal);
+extern bool jbd2_try_to_move_tail(journal_t *journal);
 
 #endif
