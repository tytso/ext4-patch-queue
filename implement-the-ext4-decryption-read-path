From: Michael Halcrow <mhalcrow@google.com>

ext4: implement the ext4 decryption the read path

Pulls in read_full_page(), modified to support decryption on read
complete.

[ XXX there is some non-decryption related patch hunks here we need to
  move around -- tytso ]

Signed-off-by: Michael Halcrow <mhalcrow@google.com>
Signed-off-by: Theodore Ts'o <tytso@mit.edu>
Signed-off-by: Ildar Muslukhov <ildarm@google.com>
---
 fs/ext4/crypto.c | 136 +++++++++++++++++++++++++++++++++++-------
 fs/ext4/ext4.h   |   1 +
 fs/ext4/file.c   |  16 ++++-
 fs/ext4/inode.c  | 177 ++++++++++++++++++++++++++++++++++++++++++++++++++++---
 fs/ext4/super.c  |   8 ++-
 5 files changed, 304 insertions(+), 34 deletions(-)

diff --git a/fs/ext4/crypto.c b/fs/ext4/crypto.c
index 80e6fac..0a4d9fb 100644
--- a/fs/ext4/crypto.c
+++ b/fs/ext4/crypto.c
@@ -27,16 +27,18 @@
 #include <linux/random.h>
 #include <linux/scatterlist.h>
 #include <linux/spinlock_types.h>
+#include <linux/key.h>
 
 #include "ext4.h"
 #include "xattr.h"
 
 /* Encryption added and removed here! (L: */
 
-mempool_t *ext4_bounce_page_pool = NULL;
+static mempool_t *ext4_bounce_page_pool = NULL;
 
-LIST_HEAD(ext4_free_crypto_ctxs);
-DEFINE_SPINLOCK(ext4_crypto_ctx_lock);
+static LIST_HEAD(ext4_free_crypto_ctxs);
+static DEFINE_SPINLOCK(ext4_crypto_ctx_lock);
+static struct ext4_encryption_key dummy_key;
 
 /**
  * ext4_release_crypto_ctx() - Releases an encryption context
@@ -79,7 +81,7 @@ void ext4_release_crypto_ctx(struct ext4_crypto_ctx *ctx)
  * Return: An allocated and initialized encryption context on success. An error
  * value or NULL otherwise.
  */
-static struct ext4_crypto_ctx *ext4_alloc_and_init_crypto_ctx(u32 mask)
+static struct ext4_crypto_ctx *ext4_alloc_and_init_crypto_ctx(gfp_t mask)
 {
 	struct ext4_crypto_ctx *ctx = kzalloc(sizeof(struct ext4_crypto_ctx),
 					      mask);
@@ -367,8 +369,8 @@ static void ext4_prep_pages_for_write(struct page *ciphertext_page,
  * Return: An allocated page with the encrypted content on success. Else, an
  * error value or NULL.
  */
-struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
-			      struct page *plaintext_page)
+static struct page *ext4_xts_encrypt(struct ext4_crypto_ctx *ctx,
+				     struct page *plaintext_page)
 {
 	struct page *ciphertext_page = ctx->bounce_page;
 	u8 xts_tweak[EXT4_XTS_TWEAK_SIZE];
@@ -473,7 +475,7 @@ struct page *ext4_encrypt(struct ext4_crypto_ctx *ctx,
  *
  * Return: Zero on success, non-zero otherwise.
  */
-int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
+static int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
 {
 	u8 xts_tweak[EXT4_XTS_TWEAK_SIZE];
 	struct ablkcipher_request *req = NULL;
@@ -514,7 +516,7 @@ int ext4_xts_decrypt(struct ext4_crypto_ctx *ctx, struct page *page)
 	ablkcipher_request_free(req);
 out:
 	if (res)
-		printk_ratelimited(KERN_ERR "%s: res = [%d]\n", __func__, res);
+		printk_ratelimited(KERN_ERR "%s: res = %d\n", __func__, res);
 	return res;
 }
 
@@ -570,11 +572,18 @@ static int ext4_get_wrapping_key_from_keyring(
 	payload = (struct encrypted_key_payload *)create_key->payload.data;
 	if (WARN_ON_ONCE(create_key->datalen !=
 			 sizeof(struct ecryptfs_auth_tok))) {
+		printk(KERN_ERR
+		       "%s: Got auth tok length %d, expected %zd\n",
+		       __func__, create_key->datalen,
+		       sizeof(struct ecryptfs_auth_tok));
 		return -EINVAL;
 	}
 	auth_tok = (struct ecryptfs_auth_tok *)(&(payload)->payload_data);
 	if (WARN_ON_ONCE(!(auth_tok->token.password.flags &
 			   ECRYPTFS_SESSION_KEY_ENCRYPTION_KEY_SET))) {
+		printk(KERN_ERR
+		       "%s: ECRYPTFS_SESSION_KEY_ENCRYPTION_KEY_SET not set in auth_tok->token.password.flags\n",
+		       __func__);
 		return -EINVAL;
 	}
 	BUILD_BUG_ON(EXT4_MAX_KEY_SIZE < EXT4_AES_256_XTS_KEY_SIZE);
@@ -662,7 +671,7 @@ static uint32_t ext4_validate_encryption_key_size(uint32_t mode, uint32_t size)
 struct ext4_hmac_result {
 	struct completion completion;
 	int res;
-} ext4_hmac_result;
+};
 
 /**
  * ext4_hmac_complete() - Completion for async HMAC
@@ -705,8 +714,11 @@ static int ext4_hmac(bool derivation, const char *key, size_t key_size,
 	int res = 0;
 
 	BUG_ON(dst_size > SHA512_DIGEST_SIZE);
-	if (IS_ERR(tfm))
+	if (IS_ERR(tfm)) {
+		printk(KERN_ERR "%s: crypto_alloc_ahash() returned %ld\n",
+		       __func__, PTR_ERR(tfm));
 		return PTR_ERR(tfm);
+	}
 	req = ahash_request_alloc(tfm, GFP_NOFS);
 	if (!req) {
 		res = -ENOMEM;
@@ -718,8 +730,11 @@ static int ext4_hmac(bool derivation, const char *key, size_t key_size,
 				   ext4_hmac_complete, &ehr);
 
 	res = crypto_ahash_setkey(tfm, key, key_size);
-	if (res)
+	if (res) {
+		printk(KERN_ERR "%s: crypto_ahash_setkey() returned %d\n",
+		       __func__, res);
 		goto out;
+	}
 	sg_init_one(&sg, src, src_size);
 	ahash_request_set_crypt(req, &sg, hmac, src_size);
 	init_completion(&ehr.completion);
@@ -729,13 +744,18 @@ static int ext4_hmac(bool derivation, const char *key, size_t key_size,
 		wait_for_completion(&ehr.completion);
 		res = ehr.res;
 	}
-	if (res)
+	if (res) {
+		printk(KERN_ERR "%s: crypto_ahash_digest() returned %d\n",
+		       __func__, res);
 		goto out;
+	}
 	memcpy(dst, hmac, dst_size);
 out:
 	crypto_free_ahash(tfm);
 	if (req)
 		ahash_request_free(req);
+	if (res)
+		printk(KERN_ERR "%s: returning %d\n", __func__, res);
 	return res;
 }
 
@@ -799,8 +819,11 @@ static int ext4_crypt_wrapper_virt(const char *enc_key, const char *iv,
 	int res = 0;
 
 	desc.tfm = crypto_alloc_blkcipher("ctr(aes)", 0, CRYPTO_ALG_ASYNC);
-	if (IS_ERR(desc.tfm))
+	if (IS_ERR(desc.tfm)) {
+		printk(KERN_ERR "%s: crypto_alloc_blkcipher() returned %ld\n",
+		       __func__, PTR_ERR(desc.tfm));
 		return PTR_ERR(desc.tfm);
+	}
 	if (!desc.tfm)
 		return -ENOMEM;
 	crypto_blkcipher_set_flags(desc.tfm, CRYPTO_TFM_REQ_WEAK_KEY);
@@ -809,12 +832,19 @@ static int ext4_crypt_wrapper_virt(const char *enc_key, const char *iv,
 	crypto_blkcipher_set_iv(desc.tfm, iv, EXT4_WRAPPING_IV_SIZE);
 	res = crypto_blkcipher_setkey(desc.tfm, enc_key,
 				      EXT4_AES_256_CTR_KEY_SIZE);
-	if (res)
+	if (res) {
+		printk(KERN_ERR "%s: crypto_blkcipher_setkey() returned %d\n",
+		       __func__, res);
 		goto out;
+	}
 	if (enc)
 		res = crypto_blkcipher_encrypt(&desc, &dst, &src, size);
 	else
 		res = crypto_blkcipher_decrypt(&desc, &dst, &src, size);
+	if (res) {
+		printk(KERN_ERR "%s: crypto_blkcipher_*crypt() returned %d\n",
+		       __func__, res);
+	}
 out:
 	crypto_free_blkcipher(desc.tfm);
 	return res;
@@ -953,8 +983,12 @@ static int ext4_wrap_key(char *wrapped_key_packet, size_t *key_packet_size,
 		return 0;
 	}
 	res = ext4_get_wrapping_key(wrapping_key, packet->sig, inode);
-	if (res)
+	if (res) {
+		ext4_error(inode->i_sb,
+			   "%s: ext4_get_wrapping_key() with packet->sig %s returned %d\n",
+			   __func__, packet->sig, res);
 		return res;
+	}
 	BUG_ON(*key_packet_size != EXT4_FULL_WRAPPED_KEY_PACKET_V0_SIZE);
 
 	/* Size, type, nonce, and IV */
@@ -970,8 +1004,12 @@ static int ext4_wrap_key(char *wrapped_key_packet, size_t *key_packet_size,
 				   packet->nonce,
 				   EXT4_DERIVATION_TWEAK_NONCE_SIZE,
 				   enc_key, EXT4_AES_256_CTR_KEY_SIZE);
-	if (res)
+	if (res) {
+		ext4_error(inode->i_sb,
+			   "%s: ext4_hmac_derive_key() returned %d\n",
+			   __func__, res);
 		goto out;
+	}
 
 	/* Wrap the data key with the wrapping encryption key */
 	*((uint32_t *)key_packet.mode) = htonl(key->mode);
@@ -984,8 +1022,12 @@ static int ext4_wrap_key(char *wrapped_key_packet, size_t *key_packet_size,
 				      EXT4_V0_SERIALIZED_KEY_SIZE, true);
 	memset(enc_key, 0, EXT4_AES_256_CTR_KEY_SIZE);
 	memset(key_packet.raw, 0, EXT4_MAX_KEY_SIZE);
-	if (res)
+	if (res) {
+		ext4_error(inode->i_sb,
+			   "%s: ext4_crypt_wrapper_virt() returned %d\n",
+			   __func__, res);
 		goto out;
+	}
 
 	/* Calculate the HMAC over the entire packet (except, of
 	 * course, the HMAC buffer at the end) */
@@ -994,8 +1036,12 @@ static int ext4_wrap_key(char *wrapped_key_packet, size_t *key_packet_size,
 				   packet->nonce,
 				   EXT4_DERIVATION_TWEAK_NONCE_SIZE,
 				   int_key, EXT4_HMAC_KEY_SIZE);
-	if (res)
+	if (res) {
+		ext4_error(inode->i_sb,
+			   "%s: ext4_hmac_derive_key() returned %d\n",
+			   __func__, res);
 		goto out;
+	}
 	BUILD_BUG_ON(EXT4_FULL_WRAPPED_KEY_PACKET_V0_SIZE < EXT4_HMAC_SIZE);
 	res = ext4_hmac_integrity(int_key, EXT4_HMAC_KEY_SIZE,
 				  wrapped_key_packet,
@@ -1006,6 +1052,8 @@ static int ext4_wrap_key(char *wrapped_key_packet, size_t *key_packet_size,
 	memset(int_key, 0, EXT4_HMAC_KEY_SIZE);
 out:
 	memset(wrapping_key, 0, EXT4_AES_256_XTS_KEY_SIZE);
+	if (res)
+		ext4_error(inode->i_sb, "%s: returning %d\n", __func__, res);
 	return res;
 }
 
@@ -1025,6 +1073,26 @@ static void ext4_generate_encryption_key(const struct dentry *dentry)
 	get_random_bytes(key->raw, key->size);
 }
 
+/*
+ * Ted lost his saving throw vs ecryptfs key management, so use a
+ * dummy key for testing purposes.  It appears the ecryptfs userspace
+ * ABI is mysteriously kconfig dependent, or there is some mysterious
+ * silent failure if you are missing some kconfig option.  This also
+ * allows us to avoid bloating the kvm-xfstests image with
+ * ecryptfs-utils.
+ */
+static void generate_dummy_key(struct inode *inode)
+{
+	int i;
+
+	dummy_key.mode = EXT4_SB(inode->i_sb)->s_default_encryption_mode;
+	dummy_key.size = ext4_encryption_key_size(dummy_key.mode);
+	for (i = 0; i < dummy_key.size; i++) {
+		dummy_key.raw[i] = "TESTKEY"[i % 7];
+	}
+}
+
+
 /**
  * ext4_set_crypto_key() - Generates and sets the encryption key for the inode
  * @dentry: The dentry for the encryption key.
@@ -1045,12 +1113,23 @@ int ext4_set_crypto_key(struct dentry *dentry)
 	struct ext4_inode_info *ei = EXT4_I(inode);
 	int res = 0;
 
+	if (test_opt2(inode->i_sb, DUMMY_ENCRYPTION)) {
+		if (unlikely(dummy_key.mode) == 0)
+			generate_dummy_key(inode);
+		ei->i_encryption_key = dummy_key;
+		return 0;
+	}
+
 try_again:
 	ext4_generate_encryption_key(dentry);
 	res = ext4_wrap_key(wrapped_key_packet, &wrapped_key_packet_size,
 			    &ei->i_encryption_key, inode);
-	if (res)
+	if (res) {
+		ext4_error(dentry->d_inode->i_sb,
+			   "%s: ext4_wrap_key() returned %d\n", __func__,
+			   res);
 		goto out;
+	}
 	root_packet[0] = EXT4_PACKET_SET_VERSION_V0;
 	BUILD_BUG_ON(EXT4_PACKET_SET_V0_MAX_SIZE !=
 		     (EXT4_PACKET_HEADER_SIZE +
@@ -1058,12 +1137,17 @@ try_again:
 	BUG_ON(sizeof(root_packet) != root_packet_size);
 	res = ext4_xattr_set(inode, EXT4_XATTR_INDEX_ENCRYPTION_METADATA, "",
 			     root_packet, root_packet_size, 0);
+	if (res) {
+		ext4_error(dentry->d_inode->i_sb,
+			   "%s: ext4_xattr_set() returned %d\n", __func__,
+			   res);
+	}
 out:
 	if (res) {
 		if (res == -EINTR)
 			goto try_again;
 		ei->i_encryption_key.mode = EXT4_ENCRYPTION_MODE_INVALID;
-		printk_ratelimited(KERN_ERR "%s: res = [%d]\n", __func__, res);
+		printk_ratelimited(KERN_ERR "%s: res = %d\n", __func__, res);
 	}
 	return res;
 }
@@ -1095,7 +1179,7 @@ static int ext4_get_root_packet(struct inode *inode, char *root_packet,
 	if (root_packet[0] != EXT4_PACKET_SET_VERSION_V0) {
 		printk_ratelimited(
 			KERN_ERR
-			"%s: Expected root packet version [%d]; got [%d]\n",
+			"%s: Expected root packet version %d; got %d\n",
 			__func__, EXT4_PACKET_SET_VERSION_V0, root_packet[0]);
 		return -EINVAL;
 	}
@@ -1117,8 +1201,16 @@ int ext4_get_crypto_key(const struct file *file)
 				   wrapped_key_packet_size);
 	struct inode *inode = file->f_mapping->host;
 	struct ext4_inode_info *ei = EXT4_I(inode);
-	int res = ext4_get_root_packet(inode, root_packet, &root_packet_size);
+	int res;
+
+	if (test_opt2(inode->i_sb, DUMMY_ENCRYPTION)) {
+		if (unlikely(dummy_key.mode) == 0)
+			generate_dummy_key(inode);
+		ei->i_encryption_key = dummy_key;
+		return 0;
+	}
 
+	res = ext4_get_root_packet(inode, root_packet, &root_packet_size);
 	if (res)
 		goto out;
 	res = ext4_unwrap_key(wrapped_key_packet,
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 11a9960..292a3a3 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1011,6 +1011,7 @@ struct ext4_inode_info {
 						      blocks */
 #define EXT4_MOUNT2_HURD_COMPAT		0x00000004 /* Support HURD-castrated
 						      file systems */
+#define EXT4_MOUNT2_DUMMY_ENCRYPTION	0x80000000 /* Use dummy encryption */
 
 #define clear_opt(sb, opt)		EXT4_SB(sb)->s_mount_opt &= \
 						~EXT4_MOUNT_##opt
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index aca7b24..6958f1a 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -200,8 +200,13 @@ static const struct vm_operations_struct ext4_file_vm_ops = {
 
 static int ext4_file_mmap(struct file *file, struct vm_area_struct *vma)
 {
+	int res;
+
 	file_accessed(file);
 	vma->vm_ops = &ext4_file_vm_ops;
+	res = ext4_get_crypto_key(file);
+	if (res == -EACCES) /* If it's encrypted and we don't have the key */
+		return res;
 	return 0;
 }
 
@@ -212,6 +217,7 @@ static int ext4_file_open(struct inode * inode, struct file * filp)
 	struct vfsmount *mnt = filp->f_path.mnt;
 	struct path path;
 	char buf[64], *cp;
+	int ret;
 
 	if (unlikely(!(sbi->s_mount_flags & EXT4_MF_MNTDIR_SAMPLED) &&
 		     !(sb->s_flags & MS_RDONLY))) {
@@ -250,11 +256,17 @@ static int ext4_file_open(struct inode * inode, struct file * filp)
 	 * writing and the journal is present
 	 */
 	if (filp->f_mode & FMODE_WRITE) {
-		int ret = ext4_inode_attach_jinode(inode);
+		ret = ext4_inode_attach_jinode(inode);
 		if (ret < 0)
 			return ret;
 	}
-	return dquot_file_open(inode, filp);
+	ret = dquot_file_open(inode, filp);
+	if (!ret) {
+		ret = ext4_get_crypto_key(filp);
+		if (ret != -EACCES)
+			ret = 0;
+	}
+	return ret;
 }
 
 /*
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 117b691..c60f15c 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -39,6 +39,7 @@
 #include <linux/ratelimit.h>
 #include <linux/aio.h>
 #include <linux/bitops.h>
+#include <linux/prefetch.h>
 
 #include "ext4_jbd2.h"
 #include "xattr.h"
@@ -781,6 +782,8 @@ struct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,
 			       ext4_lblk_t block, int create)
 {
 	struct buffer_head *bh;
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	struct ext4_crypto_ctx *ctx;
 
 	bh = ext4_getblk(handle, inode, block, create);
 	if (IS_ERR(bh))
@@ -789,8 +792,14 @@ struct buffer_head *ext4_bread(handle_t *handle, struct inode *inode,
 		return bh;
 	ll_rw_block(READ | REQ_META | REQ_PRIO, 1, &bh);
 	wait_on_buffer(bh);
-	if (buffer_uptodate(bh))
+	if (buffer_uptodate(bh)) {
+		if (ext4_is_encryption_enabled(ei)) {
+			ctx = ext4_get_crypto_ctx(false, &ei->i_encryption_key);
+			WARN_ON_ONCE(ext4_decrypt(ctx, bh->b_page));
+			ext4_release_crypto_ctx(ctx);
+		}
 		return bh;
+	}
 	put_bh(bh);
 	return ERR_PTR(-EIO);
 }
@@ -877,9 +886,8 @@ int do_journal_get_write_access(handle_t *handle,
 
 static int ext4_get_block_write_nolock(struct inode *inode, sector_t iblock,
 		   struct buffer_head *bh_result, int create);
-
 static int ext4_block_write_begin(struct page *page, loff_t pos, unsigned len,
-				  get_block_t *get_block)
+			       get_block_t *get_block)
 {
 	unsigned from = pos & (PAGE_CACHE_SIZE - 1);
 	unsigned to = from + len;
@@ -971,7 +979,6 @@ out:
 	return err;
 }
 
-
 static int ext4_write_begin(struct file *file, struct address_space *mapping,
 			    loff_t pos, unsigned len, unsigned flags,
 			    struct page **pagep, void **fsdata)
@@ -2376,7 +2383,6 @@ static int ext4_writepages(struct address_space *mapping,
 	handle_t *handle = NULL;
 	struct mpage_da_data mpd;
 	struct inode *inode = mapping->host;
-	struct ext4_inode_info *ei = EXT4_I(inode);
 	int needed_blocks, rsv_blocks = 0, ret = 0;
 	struct ext4_sb_info *sbi = EXT4_SB(mapping->host->i_sb);
 	bool done;
@@ -2393,7 +2399,7 @@ static int ext4_writepages(struct address_space *mapping,
 	if (!mapping->nrpages || !mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))
 		goto out_writepages;
 
-	if (ext4_should_journal_data(inode) || ext4_is_encryption_enabled(ei)) {
+	if (ext4_should_journal_data(inode)) {
 		struct blk_plug plug;
 
 		blk_start_plug(&plug);
@@ -2908,20 +2914,142 @@ static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
 	return generic_block_bmap(mapping, block, ext4_get_block);
 }
 
+static void ext4_completion_work(struct work_struct *work)
+{
+	struct ext4_crypto_ctx *ctx =
+		container_of(work, struct ext4_crypto_ctx, work);
+	struct page *page = ctx->control_page;
+	WARN_ON_ONCE(ext4_decrypt(ctx, page));
+	ext4_release_crypto_ctx(ctx);
+	SetPageUptodate(page);
+	unlock_page(page);
+}
+
+static int ext4_complete_cb(struct bio *bio, int res)
+{
+	struct ext4_crypto_ctx *ctx = bio->bi_cb_ctx;
+	struct page *page = ctx->control_page;
+	if (res) {
+		ext4_release_crypto_ctx(ctx);
+		unlock_page(page);
+		return res;
+	}
+	INIT_WORK(&ctx->work, ext4_completion_work);
+	queue_work(mpage_read_workqueue, &ctx->work);
+	return 0;
+}
+
+static int ext4_read_full_page(struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct buffer_head *head = create_page_buffers(page, inode, 0);
+	unsigned int blocksize = head->b_size;
+	unsigned int bbits = ilog2(blocksize);
+	sector_t iblock = (sector_t)page->index << (PAGE_CACHE_SHIFT - bbits);
+	sector_t lblock = (i_size_read(inode)+blocksize-1) >> bbits;
+	struct buffer_head *bh = head;
+	struct buffer_head *arr[MAX_BUF_PER_PAGE];
+	int nr = 0;
+	int i = 0;
+	int fully_mapped = 1;
+
+	do {
+		if (buffer_uptodate(bh))
+			continue;
+
+		if (!buffer_mapped(bh)) {
+			int err = 0;
+
+			fully_mapped = 0;
+			if (iblock < lblock) {
+				WARN_ON(bh->b_size != blocksize);
+				err = ext4_get_block(inode, iblock, bh, 0);
+				if (err)
+					SetPageError(page);
+			}
+			if (!buffer_mapped(bh)) {
+				zero_user(page, i * blocksize, blocksize);
+				if (!err)
+					set_buffer_uptodate(bh);
+				continue;
+			}
+			/*
+			 * get_block() might have updated the buffer
+			 * synchronously
+			 */
+			if (buffer_uptodate(bh))
+				continue;
+		}
+		arr[nr++] = bh;
+	} while (i++, iblock++, (bh = bh->b_this_page) != head);
+
+	if (fully_mapped)
+		SetPageMappedToDisk(page);
+
+	if (!nr) {
+		/*
+		 * All buffers are uptodate - we can set the page uptodate
+		 * as well. But not if get_block() returned an error.
+		 */
+		if (!PageError(page))
+			SetPageUptodate(page);
+		unlock_page(page);
+		return 0;
+	}
+
+	/*
+	 * Encryption requires blocksize is page size, so we should never see
+	 * more than one buffer head per page.
+	 */
+	BUG_ON(nr != 1);
+
+	/* Stage two: lock the buffers */
+	for (i = 0; i < nr; i++) {
+		bh = arr[i];
+		lock_buffer(bh);
+		mark_buffer_async_read(bh);
+	}
+
+	/*
+	 * Stage 3: start the IO.  Check for uptodateness
+	 * inside the buffer lock in case another process reading
+	 * the underlying blockdev brought it uptodate (the sct fix).
+	 */
+	for (i = 0; i < nr; i++) {
+		bh = arr[i];
+		if (buffer_uptodate(bh))
+			end_buffer_async_read(bh, 1);
+		else {
+			struct ext4_inode_info *ei = EXT4_I(inode);
+			struct ext4_crypto_ctx *ctx = ext4_get_crypto_ctx(
+				false, &ei->i_encryption_key);
+			atomic_inc(&ctx->dbg_refcnt);
+			ctx->control_page = page;
+			if (submit_bh_cb(READ, bh, ext4_complete_cb, ctx))
+				ext4_release_crypto_ctx(ctx);
+		}
+	}
+	return 0;
+}
+
 static int ext4_readpage(struct file *file, struct page *page)
 {
 	int ret = -EAGAIN;
 	struct inode *inode = page->mapping->host;
+	struct ext4_inode_info *ei = EXT4_I(inode);
 
 	trace_ext4_readpage(page);
 
 	if (ext4_has_inline_data(inode))
 		ret = ext4_readpage_inline(inode, page);
 
-	if (ret == -EAGAIN)
+	if (ext4_is_encryption_enabled(ei)) {
+		ext4_read_full_page(page);
+	} else if (ret == -EAGAIN) {
 		return mpage_readpage(page, ext4_get_block);
+	}
 
-	return ret;
+	return 0;
 }
 
 static int
@@ -2929,12 +3057,35 @@ ext4_readpages(struct file *file, struct address_space *mapping,
 		struct list_head *pages, unsigned nr_pages)
 {
 	struct inode *inode = mapping->host;
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	struct page *page = NULL;
+	unsigned page_idx;
 
 	/* If the file has inline data, no need to do readpages. */
 	if (ext4_has_inline_data(inode))
 		return 0;
 
-	return mpage_readpages(mapping, pages, nr_pages, ext4_get_block);
+	if (ext4_is_encryption_enabled(ei)) {
+		for (page_idx = 0; page_idx < nr_pages; page_idx++) {
+			page = list_entry(pages->prev, struct page, lru);
+			prefetchw(&page->flags);
+			list_del(&page->lru);
+			if (!add_to_page_cache_lru(page, mapping, page->index,
+						   GFP_KERNEL)) {
+				if (!PageUptodate(page)) {
+					ext4_read_full_page(page);
+				} else {
+					unlock_page(page);
+				}
+			}
+			page_cache_release(page);
+		}
+		BUG_ON(!list_empty(pages));
+		return 0;
+	} else {
+		return mpage_readpages(mapping, pages, nr_pages,
+				       ext4_get_block);
+	}
 }
 
 static void ext4_invalidatepage(struct page *page, unsigned int offset,
@@ -3322,8 +3473,10 @@ static int ext4_block_zero_page_range(handle_t *handle,
 	unsigned blocksize, max, pos;
 	ext4_lblk_t iblock;
 	struct inode *inode = mapping->host;
+	struct ext4_inode_info *ei = EXT4_I(inode);
 	struct buffer_head *bh;
 	struct page *page;
+	struct ext4_crypto_ctx *ctx;
 	int err = 0;
 
 	page = find_or_create_page(mapping, from >> PAGE_CACHE_SHIFT,
@@ -3379,6 +3532,12 @@ static int ext4_block_zero_page_range(handle_t *handle,
 		/* Uhhuh. Read error. Complain and punt. */
 		if (!buffer_uptodate(bh))
 			goto unlock;
+		if (ext4_is_encryption_enabled(ei)) {
+			BUG_ON(blocksize != PAGE_CACHE_SIZE);
+			ctx = ext4_get_crypto_ctx(false, &ei->i_encryption_key);
+			WARN_ON_ONCE(ext4_decrypt(ctx, page));
+			ext4_release_crypto_ctx(ctx);
+		}
 	}
 	if (ext4_should_journal_data(inode)) {
 		BUFFER_TRACE(bh, "get write access");
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index c3f4d4c..240bce4 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -1147,7 +1147,7 @@ enum {
 	Opt_inode_readahead_blks, Opt_journal_ioprio,
 	Opt_dioread_nolock, Opt_dioread_lock,
 	Opt_discard, Opt_nodiscard, Opt_init_itable, Opt_noinit_itable,
-	Opt_max_dir_size_kb, Opt_encrypt_key_sig,
+	Opt_max_dir_size_kb, Opt_encrypt_key_sig, Opt_dummy_encryption
 };
 
 static const match_table_t tokens = {
@@ -1224,6 +1224,7 @@ static const match_table_t tokens = {
 	{Opt_noinit_itable, "noinit_itable"},
 	{Opt_max_dir_size_kb, "max_dir_size_kb=%u"},
 	{Opt_encrypt_key_sig, "encrypt_key_sig=%s"},
+	{Opt_dummy_encryption, "dummy_encryption" },
 	{Opt_removed, "check=none"},	/* mount option from ext2/3 */
 	{Opt_removed, "nocheck"},	/* mount option from ext2/3 */
 	{Opt_removed, "reservation"},	/* mount option from ext2/3 */
@@ -1423,6 +1424,7 @@ static const struct mount_opts {
 	{Opt_jqfmt_vfsv1, QFMT_VFS_V1, MOPT_QFMT},
 	{Opt_max_dir_size_kb, 0, MOPT_GTE0},
 	{Opt_encrypt_key_sig, 0, MOPT_STRING},
+	{Opt_dummy_encryption, 0, 0},
 	{Opt_err, 0, 0}
 };
 
@@ -1546,6 +1548,10 @@ static int handle_mount_opt(struct super_block *sb, char *opt, int token,
 		       ECRYPTFS_SIG_SIZE_HEX);
 		sbi->s_default_encryption_wrapper_desc.wrapping_key_sig[
 			ECRYPTFS_SIG_SIZE_HEX] = '\0';
+	} else if (token == Opt_dummy_encryption) {
+		sbi->s_default_encryption_mode =
+			EXT4_ENCRYPTION_MODE_AES_256_XTS;
+		set_opt2(sb, DUMMY_ENCRYPTION);
 	} else if (token == Opt_stripe) {
 		sbi->s_stripe = arg;
 	} else if (token == Opt_resuid) {
-- 
2.1.0.rc2.206.gedb03e5


